-   **DEADLINE:** 20<sup>th</sup> June 2019


# Introduction to Machine Learning Terminologies

Learn about the following terminologies:

-   Accuracy, Precision and F-Score
-   Curse of Dimensionality
-   Bias Variance Tradoff
-   Loss Functions


### Resources

-   [Visual Introduction to Machine Learning Part 1](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/)
-   [Visual Introduction to Machine Learning Part 2](http://www.r2d3.us/visual-intro-to-machine-learning-part-2/)
-   [Introduction to Curse of Dimensionality](http://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/)


### Wikpedia Articles

-   [Wikipedia Accuracy vs Precision](https://en.wikipedia.org/wiki/Accuracy_and_precision)
-   [Wikipedia Bias vs Variance](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff)
-   [Wikipedia Curse of Dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality)


# Genetic Algorithms


## What are Genetic Algorithms?

-   [UC Davis Introduction to Genetic Algorthms](https://web.cs.ucdavis.edu/~vemuri/classes/ecs271/Genetic%20Algorithms%20Short%20Tutorial.htm)
-   [Evolutionary computation course (AEC 02 and 03 only)](https://github.com/lmarti/evolutionary-computation-course)
-   <https://lethain.com/genetic-algorithms-cool-name-damn-simple/>

-   Reference/Additional Reading : [David Goldberg's Book on Genetic Algorithms and
    Soft Computing](./David_E_Goldberg.pdf)


# Task 1

-   **DEADLINE:** **13<sup>th</sup> June 2019 (Wednesday)**


## Merge [this](https://github.com/MananSoni42/ML-SIG-2019) repository into your fork/clone

[Github Merging into fork](https://help.github.com/en/articles/merging-an-upstream-repository-into-your-fork)


## Solve [this](https://github.com/MananSoni42/ML-SIG-2019/blob/master/GA%20and%20NN/GA_task.ipynb) notebook

Implement a GA to extremize a polynomial within range.


# Neural Networks


## Visualization:

Youtube videos  - good, intuitive, in-depth
<https://youtu.be/aircAruvnKk>


## Online e-book

<http://neuralnetworksanddeeplearning.com/>


## Neural network made by google - Can understand hyper-parameter tuning

[Tensorflow Playground](http://playground.tensorflow.org)


# Task 2

DEADLINE :: 20<sup>th</sup> June 2019


## Solve fill and solve [this](https://github.com/MananSoni42/ML-SIG-2019/blob/master/GA%20and%20NN/neural_net.py) file


### Resources:

-   <https://youtu.be/aircAruvnKk>
-   <http://neuralnetworksanddeeplearning.com/>
-   <http://playground.tensorflow.org>


### Instructions:

There are 11 TODOS in this python file
Fill each one of those appropriately and you will have a working neural network
Instructions and resources have been provided wherever possible.
The implementation may not be perfect, so feel free to point out any mistakes / ask any doubts

After completing the task, some of the things you could try are (optional):

-   Implement different cost functions (binary cross-entropy)
-   Implement different activation functions (tanh, ReLU, softmax)
-   Incorporate these changes in the neural netwok code so that you can select the loss / activation function
-   Play with the hyper-paramters!


# Genetic Algorithms and Neural Networks

Combining Genetic algorithms and neural networks
Read up on these resources as. Later we will ask you to implement something similar.

-   <https://github.com/yugrocks/Genetic-Neural-Network>
-   <https://github.com/shahril96/neural-network-with-genetic-algorithm-optimizer>

